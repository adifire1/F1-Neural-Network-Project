{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f29d1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1f3fc011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.    4.   24.2  36.3 996.9   0.   38.2 296.    3.8]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('F1_4502\\LapAndWeather\\LapWeather_Australian Grand Prix')\n",
    "df = df.drop(['Time','DriverNumber', 'Unnamed: 0', 'LapNumber', 'PitOutTime', 'PitInTime', 'Sector1Time','Sector2Time','Sector3Time','Sector1SessionTime','Sector2SessionTime','Sector3SessionTime','SpeedI1','SpeedI2','SpeedST','IsPersonalBest','FreshTyre','Stint','SpeedFL','LapStartTime','Team','Driver','TrackStatus','IsAccurate'], axis=1)\n",
    "df = df.dropna()\n",
    "dfOutput = df['LapTime']\n",
    "dfInput = df.drop(['LapTime'], axis=1)\n",
    "dfInput = dfInput.replace({'ULTRASOFT': 0, 'SUPERSOFT': 1, 'SOFT': 2,})\n",
    "train_inputs = dfInput.to_numpy()\n",
    "train_targets = dfOutput.to_numpy()\n",
    "for i in range(len(train_targets)):\n",
    "    train_targets[i] = train_targets[i].replace('0 days ', '')\n",
    "actual_train_targets = []\n",
    "for time in train_targets:\n",
    "    td = parse(time) - parse('00:00:00')\n",
    "    seconds = td.total_seconds()\n",
    "    actual_train_targets.append(seconds)\n",
    "train_inputs = train_inputs.astype('float64')\n",
    "print(train_inputs[0])\n",
    "train_targets = np.array(actual_train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3a5f5a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch     1: 0.192\n",
      "Loss after mini-batch    11: 1.865\n",
      "Loss after mini-batch    21: 1.868\n",
      "Loss after mini-batch    31: 1.860\n",
      "Loss after mini-batch    41: 1.817\n",
      "Loss after mini-batch    51: 1.868\n",
      "Loss after mini-batch    61: 1.826\n",
      "Loss after mini-batch    71: 1.837\n",
      "Loss after mini-batch    81: 1.855\n",
      "Loss after mini-batch    91: 1.861\n",
      "Starting epoch 2\n",
      "Loss after mini-batch     1: 0.177\n",
      "Loss after mini-batch    11: 1.854\n",
      "Loss after mini-batch    21: 1.849\n",
      "Loss after mini-batch    31: 1.852\n",
      "Loss after mini-batch    41: 1.846\n",
      "Loss after mini-batch    51: 1.859\n",
      "Loss after mini-batch    61: 1.843\n",
      "Loss after mini-batch    71: 1.826\n",
      "Loss after mini-batch    81: 1.851\n",
      "Loss after mini-batch    91: 1.839\n",
      "Starting epoch 3\n",
      "Loss after mini-batch     1: 0.199\n",
      "Loss after mini-batch    11: 1.875\n",
      "Loss after mini-batch    21: 1.809\n",
      "Loss after mini-batch    31: 1.842\n",
      "Loss after mini-batch    41: 1.845\n",
      "Loss after mini-batch    51: 1.803\n",
      "Loss after mini-batch    61: 1.819\n",
      "Loss after mini-batch    71: 1.821\n",
      "Loss after mini-batch    81: 1.827\n",
      "Loss after mini-batch    91: 1.850\n",
      "Starting epoch 4\n",
      "Loss after mini-batch     1: 0.191\n",
      "Loss after mini-batch    11: 1.861\n",
      "Loss after mini-batch    21: 1.797\n",
      "Loss after mini-batch    31: 1.830\n",
      "Loss after mini-batch    41: 1.807\n",
      "Loss after mini-batch    51: 1.819\n",
      "Loss after mini-batch    61: 1.844\n",
      "Loss after mini-batch    71: 1.795\n",
      "Loss after mini-batch    81: 1.790\n",
      "Loss after mini-batch    91: 1.802\n",
      "Starting epoch 5\n",
      "Loss after mini-batch     1: 0.188\n",
      "Loss after mini-batch    11: 1.801\n",
      "Loss after mini-batch    21: 1.802\n",
      "Loss after mini-batch    31: 1.808\n",
      "Loss after mini-batch    41: 1.810\n",
      "Loss after mini-batch    51: 1.809\n",
      "Loss after mini-batch    61: 1.764\n",
      "Loss after mini-batch    71: 1.807\n",
      "Loss after mini-batch    81: 1.789\n",
      "Loss after mini-batch    91: 1.756\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class f1Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "  def __init__(self, X, y, scale_data=True):\n",
    "    if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "      # Apply scaling if necessary\n",
    "      if scale_data:\n",
    "          X = StandardScaler().fit_transform(X)\n",
    "      self.X = torch.from_numpy(X)\n",
    "      self.y = torch.from_numpy(y)\n",
    "\n",
    "  def __len__(self):\n",
    "      return len(self.X)\n",
    "\n",
    "  def __getitem__(self, i):\n",
    "      return self.X[i], self.y[i]\n",
    "      \n",
    "\n",
    "class MLP(nn.Module):\n",
    "  '''\n",
    "    Multilayer Perceptron for regression.\n",
    "  '''\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Linear(9, 64),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(64, 32),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(32, 1)\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    '''\n",
    "      Forward pass\n",
    "    '''\n",
    "    return self.layers(x)\n",
    "\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "  \n",
    "  # Set fixed random number seed\n",
    "  torch.manual_seed(42)\n",
    "  \n",
    "  # Load Boston dataset\n",
    "  \n",
    "  # Prepare Boston dataset\n",
    "  dataset = f1Dataset(train_inputs, train_targets)\n",
    "  trainloader = torch.utils.data.DataLoader(dataset, batch_size=10, shuffle=True, num_workers=0)\n",
    "  \n",
    "  # Initialize the MLP\n",
    "  mlp = MLP()\n",
    "  \n",
    "  # Define the loss function and optimizer\n",
    "  loss_function = nn.L1Loss()\n",
    "  optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)\n",
    "  \n",
    "  # Run the training loop\n",
    "  for epoch in range(0, 5): # 5 epochs at maximum\n",
    "    \n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "    \n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "    \n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "      \n",
    "      # Get and prepare inputs\n",
    "      inputs, targets = data\n",
    "      inputs, targets = inputs.float(), targets.float()\n",
    "      targets = targets.reshape((targets.shape[0], 1))\n",
    "      \n",
    "      # Zero the gradients\n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      # Perform forward pass\n",
    "      outputs = mlp(inputs)\n",
    "      \n",
    "      # Compute loss\n",
    "      loss = loss_function(outputs, targets)\n",
    "      \n",
    "      # Perform backward pass\n",
    "      loss.backward()\n",
    "      \n",
    "      # Perform optimization\n",
    "      optimizer.step()\n",
    "      \n",
    "      # Print statistics\n",
    "      current_loss += loss.item()\n",
    "      if i % 10 == 0:\n",
    "          print('Loss after mini-batch %5d: %.3f' %\n",
    "                (i + 1, current_loss / 500))\n",
    "          current_loss = 0.0\n",
    "\n",
    "  # Process is complete.\n",
    "  print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3e58310d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-95511c8c2c71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-112-95511c8c2c71>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "#NOT BEING USED RIGHT NOW #######\n",
    "##########################\n",
    "\n",
    "# # Define the model\n",
    "# class NeuralNet(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size):\n",
    "#         super(NeuralNet, self).__init__()\n",
    "#         self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.fc2 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         out = self.fc1(x)\n",
    "#         out = self.relu(out)\n",
    "#         out = self.fc2(out)\n",
    "#         return out\n",
    "\n",
    "# # Create the model with input size and hidden layer size\n",
    "# input_size = 9\n",
    "# hidden_size = 16\n",
    "# model = NeuralNet(input_size, hidden_size)\n",
    "\n",
    "# # Define the loss function and optimizer\n",
    "# criterion = nn.MSELoss()\n",
    "# learning_rate = 0.01\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Train the model\n",
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Convert input and target data to PyTorch tensors\n",
    "#     inputs = torch.from_numpy(train_inputs)\n",
    "#     targets = torch.from_numpy(train_targets)\n",
    "\n",
    "#     # Forward pass\n",
    "#     outputs = model(inputs)\n",
    "#     loss = criterion(outputs, targets)\n",
    "\n",
    "#     # Backward and optimize\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     if (epoch+1) % 10 == 0:\n",
    "#         print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# # # Test the model\n",
    "# # model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "# # with torch.no_grad():\n",
    "# #     inputs = torch.from_numpy(test_inputs)\n",
    "# #     targets = torch.from_numpy(test_targets)\n",
    "# #     outputs = model(inputs)\n",
    "# #     loss = criterion(outputs, targets)\n",
    "# #     print(f'Test Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8e50d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
